<link rel="stylesheet" href="../../assets/css/style.css">
[Back to Portfolio](../../index.html)

# Movie Review Sentiment Pipeline ğŸ¬ğŸ“

**Building a sentiment analysis pipeline for IMDb movie reviews.** This project classifies reviews as positive or negative by applying NLP preprocessing, vectorization and supervised modeling. We compare classical machine learning against preâ€‘trained transformer models to demonstrate the tradeâ€‘offs between accuracy and runtime.

> **Headline:** Fineâ€‘tuned BERT reaches **89â€¯% accuracy**, outperforming classical models by 6Â percentage points.

---

## ğŸš¦ Project Status

| Component | State |
|-----------|-------|
| White paper (PDF) | âœ… in repository |
| 5â€‘minute summary slides | âœ… in repository |
| Code & notebooks | âœ… completed |
| Sentiment classifier | âœ… completed |

---

## ğŸ” Research Questions

1. **Preprocessing** â€“ Which text cleaning and tokenization steps most improve model performance?
2. **Model comparison** â€“ How do logistic regression, SVM, LSTM and BERT compare on IMDb review sentiment?
3. **Interpretability** â€“ Which words drive positive and negative classifications according to model explanations?

---

## ğŸ“Š Key Findings

| Insight | Evidence |
|---------|----------|
| Stopword removal and bigram tokenization improve logistic regression accuracy by 4Â pts | `notebooks/01_EDA.ipynb` |
| BERT fineâ€‘tuned for 3Â epochs achieves 89â€¯% accuracy vs SVM 83â€¯% | `results/model_accuracy.png` |
| Model explanations highlight adjectives (â€œbrilliantâ€, â€œterribleâ€) as key contributors | `results/word_importance.png` |
| Overly long reviews degrade LSTM performance; truncation yields better generalization | `notebooks/02_LSTM.ipynb` |

---

## ğŸ—‚ï¸ Repository Layout

```
projects/movie-review-sentiment/
â”œâ”€â”€ data/               # IMDb train/test sets
â”œâ”€â”€ notebooks/          # preprocessing, model training, evaluation
â”œâ”€â”€ results/            # plots and confusion matrices
â”œâ”€â”€ report/             # white paper and slides
â””â”€â”€ README.md
```

---

## âš™ï¸ Quick Start

```bash
 git clone https://github.com/your-org/Data-Sci-Portfolios.git
 cd Data-Sci-Portfolios/projects/movie-review-sentiment

 conda env create -f environment.yml
 conda activate movie-sentiment

 make all
```

`make all` downloads the IMDb dataset, cleans the text, fits baseline and advanced models, and generates evaluation results.

---

## ğŸ› ï¸ Methodology at a Glance

| Stage | Tool / Model |
|-------|--------------|
| Ingest | IMDb Large Movie Review dataset |
| Preprocess | lowercasing, stopword removal, tokenization |
| Vectorize | TFâ€‘IDF, word embeddings |
| Models | Logistic regression, SVM, LSTM, BERT |
| Evaluate | Accuracy, precision/recall, confusion matrix |

---

## ğŸ“ˆ Reproduce Every Figure

```bash
 jupyter nbconvert --execute --to notebook --inplace notebooks/01_EDA.ipynb
 jupyter nbconvert --execute --to notebook --inplace notebooks/02_Modeling.ipynb
```

---

## ğŸ›¡ï¸ Ethical & Fairness Guardâ€‘Rails

* Reviews are publicly available; no PII collected.
* Models should not be used to make decisions about individuals; they measure aggregated sentiment.

---

## ğŸ¤ Contributing

See `CONTRIBUTING.md` to contribute.

---

## ğŸ“œ License

MIT License. IMDb review data subject to its own terms.

---

## ğŸ“š References

- Maas A.Â etâ€¯al.Â (2011) *Learning Word Vectors for Sentiment Analysis.*
- Devlin J.Â etâ€¯al.Â (2018) *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.*