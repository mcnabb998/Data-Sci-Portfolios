
# Amazon â€œStars vs Sentimentâ€ ğŸ“¦â­ï¸ğŸ“

**A complete investigation of how often Amazon star ratings disagree with the
written sentiment of the reviews that accompany them.**  
Using one million Clothing â€¢ Shoes â€¢ Jewelry reviews from the 2023 Amazon
corpus, we measure *sentiment drift*, visualise its distribution, and show how a
simple **divergence score** can power trust dashboards, seller alerts, and
spam-detection pipelines.

> **Headline:** 64â€¯% of reviews are selfâ€‘consistent, yet **11â€¯% differ by more
> than one full star** between text and ratingâ€”enough to mislead buyers and
> skew ranking models.

---

## ğŸš¦ Project Status

| Component | State |
|-----------|-------|
| White paper (PDF) | â€” |
| 10â€‘slide video (7â€¯min) | â€” |
| Code & notebooks | âœ… pushed to `main` |
| MiniLM starâ€‘prediction fineâ€‘tune | ğŸ”„ optional â€“ planned Q4â€¯2025 |

---

## ğŸ” Research Questions

1. **Frequency** â€“ How often does textual sentiment disagree with star ratings?  
2. **Drivers** â€“ Do helpful votes, verified purchases, or brand patterns
   explain high drift?  
3. **Prediction** â€“ Can the residual between text sentiment and expected stars
   signal review manipulation?

---

## ğŸ“Š Key Findings

| Insight | Evidence |
|---------|----------|
| 64â€¯% of reviews align (divergence â‰ˆâ€¯0) | `results/divergence_hist.png` |
| **11â€¯%** exceed Â±â€¯1 divergence â†’ strong misâ€‘match | histogram tails |
| Average drift **+â€¯0â€¯.04** â†’ mild positivity bias | whiteâ€‘paper Â§â€¯6 |
| Verifiedâ€‘purchase reviews show 40â€¯% lower drift odds | AppendixÂ CÂ Q4 |
| DivergenceÂ +Â meta features â‡’ precisionÂ 0.55 for spam | AppendixÂ CÂ Q7 |

_Divergence = sentiment score âˆ’ normalised star, where starsÂ 1â€‘5 map to
âˆ’1â€¦+1._

---

## ğŸ—‚ï¸ Repository Layout
```
.
â”œâ”€â”€ data/                         # parquet slices & model artefacts
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_EDA.ipynb             # sentiment, divergence, figures
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ get_data.py              # streaming download + sample
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ star_counts.png
â”‚   â”œâ”€â”€ divergence_hist.png
â”‚   â”œâ”€â”€ polarity_vs_rating.png
â”‚   â””â”€â”€ helpful_vs_divergence.png
â”œâ”€â”€ environment.yml
â””â”€â”€ README.md                    # this file
```
---

## âš™ï¸ QuickÂ Start
```bash
git clone https://github.com/<yourâ€‘org>/stars-vs-sentiment.git
cd stars-vs-sentiment

# 1.  Create environment
conda env create -f environment.yml
conda activate stars-sentiment

# 2.  Endâ€‘toâ€‘end run  (â‰ˆâ€¯15Â min CPU â€¢Â 4Â min single A10 GPU)
make all
```
`make all` streams a oneâ€‘millionâ€‘row sample, scores sentiment with DistilBERT,
computes divergence, and regenerates every figure under `results/`.

---

## ğŸ› ï¸ Methodology at a Glance

| Stage | Tool / Model |
|-------|--------------|
| Ingest | HF datasets â†’ 1â€¯M stratified rows |
| Clean  | HTML strip â€¢ emoji removal â€¢ language filter |
| Sentiment | DistilBERT SSTâ€‘2 (binary Â±â€¯1) |
| Divergence | `sentiment âˆ’ (starâˆ’3)/2` |
| EDA | pandas & seaborn |
| Spam test | XGBoost on divergence + length + accountâ€‘age |
| Compute | CPUÂ 15â€¯min / A10Â GPUÂ 4â€¯min |

Full detailsâ€”including assumptions, limitations, and ethical controlsâ€”are in
SectionsÂ 5â€‘14 of the PDF.

---

## ğŸ“ˆ Reproduce Every Figure
```bash
jupyter nbconvert --execute --to notebook   --inplace notebooks/01_EDA.ipynb
```
All PNGs refresh under `results/`, and the notebook records runtime logs.

---

## ğŸ›¡ï¸ Ethical & Fairness Guardâ€‘Rails
* Reviewer IDs hashed; no PII stored.  
* Fairness dashboard flags any subgroup with drift >â€¯0â€¯.05.  
* Divergence badge shown only after 30 reviews to deter gaming.  
* Human moderation required before penalties.

---

## ğŸ¤ Contributing
Bug reports and pull requests welcomeâ€”see `CONTRIBUTING.md`.

---

## ğŸ“œ License
* Code â€” MIT License  
* Review text â€” Amazon Research License (nonâ€‘commercial)

---

## ğŸ“š References
HouÂ Y.Â etâ€¯al.Â (2024) *Bridging Language and Items for Retrieval and Recommendation.* arXivÂ 2403.03952  
MukherjeeÂ A.Â etâ€¯al.Â (2013) *What Yelp Fake Review Filter Might Be Doing?* WWW Companion  
SanhÂ V.Â etâ€¯al.Â (2019) *DistilBERT, a Distilled Version of BERT.* arXivÂ 1910.01108  
